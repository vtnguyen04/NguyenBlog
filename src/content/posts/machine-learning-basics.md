---
title: "Machine Learning c∆° b·∫£n cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu"
published: 2025-06-29
description: "H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ machine learning t·ª´ l√Ω thuy·∫øt ƒë·∫øn th·ª±c h√†nh"
tags: [Machine Learning, AI, Python, Scikit-learn, Data Science]
category: "AI & Machine Learning"
draft: false
lang: "vi"
---

## M·ª•c l·ª•c

1. [C√°c lo·∫°i Machine Learning](#1-c√°c-lo·∫°i-machine-learning)
    - [Supervised Learning (H·ªçc c√≥ gi√°m s√°t)](#supervised-learning-h·ªçc-c√≥-gi√°m-s√°t)
    - [Unsupervised Learning (H·ªçc kh√¥ng gi√°m s√°t)](#unsupervised-learning-h·ªçc-kh√¥ng-gi√°m-s√°t)
2. [C√°c thu·∫≠t to√°n c∆° b·∫£n](#2-c√°c-thu·∫≠t-to√°n-c∆°-b·∫£n)
    - [Linear Regression (H·ªìi quy tuy·∫øn t√≠nh)](#linear-regression-h·ªìi-quy-tuy·∫øn-t√≠nh)
    - [Logistic Regression (H·ªìi quy logistic)](#logistic-regression-h·ªìi-quy-logistic)
    - [Decision Tree (C√¢y quy·∫øt ƒë·ªãnh)](#decision-tree-c√¢y-quy·∫øt-ƒë·ªãnh)
3. [X·ª≠ l√Ω d·ªØ li·ªáu](#3-x·ª≠-l√Ω-d·ªØ-li·ªáu)
    - [Data Preprocessing](#data-preprocessing)
    - [Feature Engineering](#feature-engineering)
4. [Cross-Validation](#4-cross-validation)
5. [Hyperparameter Tuning](#5-hyperparameter-tuning)
6. [Model Evaluation](#6-model-evaluation)
7. [Deep Learning v·ªõi TensorFlow](#7-deep-learning-v·ªõi-tensorflow)
8. [K·∫øt lu·∫≠n](#k·∫øt-lu·∫≠n)

# Machine Learning c∆° b·∫£n cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu 

Machine Learning (ML) l√† m·ªôt nh√°nh c·ªßa Artificial Intelligence (AI) cho ph√©p m√°y t√≠nh h·ªçc v√† c·∫£i thi·ªán t·ª´ d·ªØ li·ªáu m√† kh√¥ng c·∫ßn ƒë∆∞·ª£c l·∫≠p tr√¨nh r√µ r√†ng. Trong b√†i vi·∫øt n√†y, t√¥i s·∫Ω gi·ªõi thi·ªáu nh·ªØng kh√°i ni·ªám c∆° b·∫£n v√† th·ª±c h√†nh v·ªõi Python.

## 1. C√°c lo·∫°i Machine Learning

### Supervised Learning (H·ªçc c√≥ gi√°m s√°t)

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# V√≠ d·ª•: D·ª± ƒëo√°n gi√° nh√†
# D·ªØ li·ªáu m·∫´u
data = {
    'di·ªán_t√≠ch': [100, 150, 200, 250, 300],
    's·ªë_ph√≤ng': [2, 3, 3, 4, 4],
    'gi√°': [500, 750, 1000, 1250, 1500]
}

df = pd.DataFrame(data)
X = df[['di·ªán_t√≠ch', 's·ªë_ph√≤ng']]
y = df['gi√°']

# Chia d·ªØ li·ªáu
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Hu·∫•n luy·ªán model
model = LinearRegression()
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)

# ƒê√°nh gi√°
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'MSE: {mse:.2f}')
print(f'R¬≤: {r2:.2f}')
```

### Unsupervised Learning (H·ªçc kh√¥ng gi√°m s√°t)

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# V√≠ d·ª•: Ph√¢n c·ª•m kh√°ch h√†ng
# D·ªØ li·ªáu m·∫´u
customer_data = np.array([
    [25, 50000],  # [tu·ªïi, thu nh·∫≠p]
    [30, 60000],
    [35, 70000],
    [40, 80000],
    [45, 90000],
    [50, 100000]
])

# Chu·∫©n h√≥a d·ªØ li·ªáu
scaler = StandardScaler()
scaled_data = scaler.fit_transform(customer_data)

# Ph√¢n c·ª•m
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Visualize
plt.scatter(customer_data[:, 0], customer_data[:, 1], 
           c=clusters, cmap='viridis')
plt.xlabel('Tu·ªïi')
plt.ylabel('Thu nh·∫≠p')
plt.title('Ph√¢n c·ª•m kh√°ch h√†ng')
plt.show()
```

## 2. C√°c thu·∫≠t to√°n c∆° b·∫£n

### Linear Regression (H·ªìi quy tuy·∫øn t√≠nh)

```python
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# T·∫°o d·ªØ li·ªáu m·∫´u
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2 * X + 1 + np.random.randn(100, 1) * 0.5

# Hu·∫•n luy·ªán model
model = LinearRegression()
model.fit(X, y)

# D·ª± ƒëo√°n
y_pred = model.predict(X)

# Visualize
plt.scatter(X, y, alpha=0.5, label='D·ªØ li·ªáu th·ª±c')
plt.plot(X, y_pred, color='red', label='D·ª± ƒëo√°n')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()

print(f'Coefficient: {model.coef_[0][0]:.2f}')
print(f'Intercept: {model.intercept_[0]:.2f}')
```

### Logistic Regression (H·ªìi quy logistic)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# D·ªØ li·ªáu m·∫´u: D·ª± ƒëo√°n spam email
X = np.array([
    [1, 0, 1, 0],  # [c√≥_link, c√≥_s·ªë_ƒëi·ªán_tho·∫°i, c√≥_t·ª´_kh·∫©n, c√≥_vi·∫øt_hoa]
    [0, 1, 0, 1],
    [1, 1, 1, 1],
    [0, 0, 0, 0],
    [1, 0, 0, 1],
    [0, 1, 1, 0]
])

y = np.array([1, 1, 1, 0, 0, 0])  # 1: spam, 0: kh√¥ng spam

# Hu·∫•n luy·ªán model
model = LogisticRegression()
model.fit(X, y)

# D·ª± ƒëo√°n
y_pred = model.predict(X)

# ƒê√°nh gi√°
accuracy = accuracy_score(y, y_pred)
print(f'ƒê·ªô ch√≠nh x√°c: {accuracy:.2f}')
print(classification_report(y, y_pred))
```

### Decision Tree (C√¢y quy·∫øt ƒë·ªãnh)

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# D·ªØ li·ªáu m·∫´u: D·ª± ƒëo√°n mua h√†ng
data = {
    'tu·ªïi': [25, 35, 45, 55, 25, 35, 45, 55],
    'thu_nh·∫≠p': ['th·∫•p', 'th·∫•p', 'cao', 'cao', 'cao', 'th·∫•p', 'cao', 'th·∫•p'],
    'mua_h√†ng': [0, 0, 1, 1, 1, 0, 1, 0]
}

df = pd.DataFrame(data)

# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['thu_nh·∫≠p_encoded'] = le.fit_transform(df['thu_nh·∫≠p'])

X = df[['tu·ªïi', 'thu_nh·∫≠p_encoded']]
y = df['mua_h√†ng']

# Hu·∫•n luy·ªán model
model = DecisionTreeClassifier(random_state=42)
model.fit(X, y)

# Visualize c√¢y quy·∫øt ƒë·ªãnh
plt.figure(figsize=(10, 8))
plot_tree(model, feature_names=['Tu·ªïi', 'Thu nh·∫≠p'], 
          class_names=['Kh√¥ng mua', 'Mua'], filled=True)
plt.show()
```

## 3. X·ª≠ l√Ω d·ªØ li·ªáu

### Data Preprocessing

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

# T·∫°o d·ªØ li·ªáu m·∫´u v·ªõi missing values
data = {
    'tu·ªïi': [25, 30, None, 35, 40],
    'thu_nh·∫≠p': [50000, 60000, 70000, None, 90000],
    'th√†nh_ph·ªë': ['H√† N·ªôi', 'TP.HCM', 'ƒê√† N·∫µng', 'H√† N·ªôi', None],
    'mua_h√†ng': [1, 0, 1, 0, 1]
}

df = pd.DataFrame(data)

# X·ª≠ l√Ω missing values
# S·ªë: thay b·∫±ng median
imputer_num = SimpleImputer(strategy='median')
df[['tu·ªïi', 'thu_nh·∫≠p']] = imputer_num.fit_transform(df[['tu·ªïi', 'thu_nh·∫≠p']])

# Categorical: thay b·∫±ng mode
imputer_cat = SimpleImputer(strategy='most_frequent')
df['th√†nh_ph·ªë'] = imputer_cat.fit_transform(df[['th√†nh_ph·ªë']])

# Encode categorical variables
le = LabelEncoder()
df['th√†nh_ph·ªë_encoded'] = le.fit_transform(df['th√†nh_ph·ªë'])

print("D·ªØ li·ªáu sau khi x·ª≠ l√Ω:")
print(df)
```

### Feature Engineering

```python
# T·∫°o features m·ªõi
df['tu·ªïi_nh√≥m'] = pd.cut(df['tu·ªïi'], bins=[0, 30, 40, 100], 
                         labels=['tr·∫ª', 'trung ni√™n', 'cao tu·ªïi'])

# One-hot encoding
df_encoded = pd.get_dummies(df, columns=['tu·ªïi_nh√≥m'])

# Scaling
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_encoded[['tu·ªïi', 'thu_nh·∫≠p']])

print("Features sau khi engineering:")
print(df_encoded.head())
```

## 4. Cross-Validation

```python
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

# S·ª≠ d·ª•ng cross-validation ƒë·ªÉ ƒë√°nh gi√° model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5)

print(f'ƒê·ªô ch√≠nh x√°c trung b√¨nh: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})')
```

## 5. Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV

# T√¨m hyperparameters t·ªët nh·∫•t
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10]
}

model = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X, y)

print(f'Best parameters: {grid_search.best_params_}')
print(f'Best score: {grid_search.best_score_:.3f}')
```

## 6. Model Evaluation

```python
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve
import seaborn as sns

# Confusion Matrix
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('Th·ª±c t·∫ø')
plt.xlabel('D·ª± ƒëo√°n')
plt.show()

# ROC Curve
y_pred_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
```

## 7. Deep Learning v·ªõi TensorFlow

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# T·∫°o neural network ƒë∆°n gi·∫£n
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Hu·∫•n luy·ªán
history = model.fit(X_train, y_train,
                    epochs=100,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=0)

# Visualize training
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
```

## K·∫øt lu·∫≠n

Machine Learning l√† m·ªôt lƒ©nh v·ª±c r·ªông l·ªõn v√† th√∫ v·ªã. ƒê·ªÉ th√†nh c√¥ng trong lƒ©nh v·ª±c n√†y, b·∫°n c·∫ßn:

### 1. **N·ªÅn t·∫£ng v·ªØng ch·∫Øc**
- To√°n h·ªçc: Linear Algebra, Calculus, Statistics
- L·∫≠p tr√¨nh: Python, R, SQL
- Ki·∫øn th·ª©c domain: Hi·ªÉu r√µ lƒ©nh v·ª±c ·ª©ng d·ª•ng

### 2. **Th·ª±c h√†nh th∆∞·ªùng xuy√™n**
- L√†m c√°c project th·ª±c t·∫ø
- Tham gia competitions (Kaggle, etc.)
- ƒê·ªçc papers v√† implement

### 3. **C·∫≠p nh·∫≠t xu h∆∞·ªõng**
- Deep Learning
- Reinforcement Learning
- AutoML
- MLOps

### 4. **Tools v√† Frameworks**
- **Scikit-learn**: ML c∆° b·∫£n
- **TensorFlow/PyTorch**: Deep Learning
- **Pandas/NumPy**: Data manipulation
- **Matplotlib/Seaborn**: Visualization

H√£y b·∫Øt ƒë·∫ßu v·ªõi nh·ªØng kh√°i ni·ªám c∆° b·∫£n v√† d·∫ßn d·∫ßn ti·∫øn t·ªõi c√°c thu·∫≠t to√°n ph·ª©c t·∫°p h∆°n! üöÄ 